[{"content":"TL;DR ⏱️ Long story short: 90%-out-of-the-blue chance that you just don\u0026rsquo;t.\nIf you want more details about our story, then feel free to keep reading 👓⬇️.\nContext 🖼️ A not-so-long-while🦖 ago🦕 we had some alerts📟🔔 in our dedicated Slack Channel💬, triggered every now and then when a particular HangFire job was running:\n\u0026#34;XXXXXXX DataLoader\u0026#34; \u0026#34;Production\u0026#34; Error 2021-01-22 09:04 SpanId: \u0026#34;xxxxxxxxxxxxx\u0026#34; TraceId: \u0026#34;xxxxxxxxxxxxx\u0026#34; An unhandled exception occurred during the Hangfire job. Depth 0: System.AggregateException One or more errors occurred. (The operation has timed out.) Depth 1: System.TimeoutException The operation has timed out. At first, it was just a RabbitMQ thingy\u0026hellip; 🐇 We then looked at our logs in Kibana🔍, just to find out that there was actually an issue related to opening a RabbitMQ Channel📡:\nSystem.AggregateException One or more errors occurred. (The operation has timed out.) System.AggregateException: One or more errors occurred. (The operation has timed out.) ---\u0026gt; System.TimeoutException: The operation has timed out. at RabbitMQ.Util.BlockingCell`1.WaitForValue(TimeSpan timeout) at RabbitMQ.Client.Impl.SimpleBlockingRpcContinuation.GetReply(TimeSpan timeout) at RabbitMQ.Client.Impl.ModelBase.ModelRpc(MethodBase method, ContentHeaderBase header, Byte[] body) at RabbitMQ.Client.Framing.Impl.Model._Private_ChannelOpen(String outOfBand) at RabbitMQ.Client.Framing.Impl.AutorecoveringConnection.CreateNonRecoveringModel() at RabbitMQ.Client.Framing.Impl.AutorecoveringConnection.CreateModel() [...] Googling🔍 around🗺️ didn\u0026rsquo;t yield too many results at first, even if we found a bunch of (remotely) related issues on GitHub🗃️:\n Time out exception on CreateModel in high volume scenario Using blocking publisher confirms with concurrent publishers Opening a channel inside a consumer interface callback times out waiting for continuation Timeout exception when trying to declare a queue (or exchange)  The Process ⚙️ Since there wasn\u0026rsquo;t really a clear solution out of our prelimenary investigations, it was time to step back and see a slightly bigger picture about what the Hangfire job was doing overall:\n Request data from an API💱 Create groups🏘️ based on some predicates Process the groups as commands in our PostgreSQL-based🐘 event store and then persist💾 the newly decided/generated events Publish💾 the events to RabbitMQ🐇  Oopsie\u0026hellip; seems like PostgreSQL 🐘 is also failing! Truth to be told🧙‍♀️, there was also an issue occuring during the PostgreSQL persistence step as well🔎:\nAn unhandled exception has occurred while executing the request. System.AggregateException: One or more errors occurred. (One or more errors occurred. (One or more errors occurred. (One or m ore errors occurred. (Exception while reading from stream)))) ---\u0026gt; System.AggregateException: One or more errors occurred. (One or more errors occurred. (One or more errors occurred. (Ex ception while reading from stream))) ---\u0026gt; System.AggregateException: One or more errors occurred. (One or more errors occurred. (Exception while reading from str eam)) ---\u0026gt; System.AggregateException: One or more errors occurred. (Exception while reading from stream) ---\u0026gt; Npgsql.NpgsqlException (0x80004005): Exception while reading from stream ---\u0026gt; System.TimeoutException: Timeout during reading attempt RabbitMQ🐇 + Multithreading 🧵: A Love Story 💞\u0026hellip; Nope❌ Another pesty little detail🦠, is that we tried to speed up⏩ the RabbitMQ publishing process using PSeq and the PublishBatch API of the official RabbitMQ .NET client.\nThe thing you see, is that during our first iteration🧗‍♀️ when implementing this service (and more generally-speaking when we were implementing the RabbitMQ support in our codebase), we already have experienced issues while experimenting with IModel in a concurrent fashion (i.e. multithreading) which obviously turned out to be a rather infamously well-known concern👩‍🎤. So much, that there is in fact, a section of the official RabbitMQ dedicated about it:\n As a rule of thumb, IModel instance usage by more than one thread simultaneously should be avoided. Application code should maintain a clear notion of thread ownership for IModel instances.\n So here is the catch, we were not using EasyNetQ, or any abstraction layer on top of the official RabbitMQ .NET client, and something like the management of a single instance of IModel per thread was suboptimal to put it mildly, i.e. we were creating a new IModel instance everytime we needed to send a batch of messages wich at some point started to create a bottleneck (i.e. several instantiations per thread🧵).\nTbf, it was already a source of struggles🏋️‍♀️, actually we\u0026rsquo;ve noticed that our implementation was much slower🐌 at sending messages than the EasyNetQ abstraction\u0026hellip; and one of the reasons was the way EasyNetQ achieved this one single IModel instance per thread policy, in essence the number of IModel instances is kept to a bare minimum. Some of the important building blocks are listed below:\n AsyncLock RabbitAdvancedBus PersistentChannel  Anyway, my colleague Yazide Boujlil managed to come up with the explanations below:\n For each client connection, we have a socket which is shared between the threads via a .NET Channel that acts as some sort of buffer. Several threads can write to it, but only a single thread will read what is there and therefore will eventually write data to the relevant socket.\nThe RMQ channels (IModel) are merely the byproducts🐤 of the RMQ connection. Each message which passes through the socket contains the ID of the channel to which it is attached. To declare a channel you need a small blocking RPC upon which we have experienced our annoying timeouts⏰.\nNow it turns out that the server can actually put connections🔌 that publish too quickly🚅 (compared to consumers) in flow control mode, i.e. aka on hold, like putting them on ice🧊, and block and unblock the connection to limit the flow of messages🚧.\nNow what is happening, in my opinion, is that we ended up in a situation where we overload both the server and the client. The server goes into a flow control mode which does not allow the client to empty its buffer and therefore any new channel creation is blocked until the cache is cleared. As there is a timeout on the channel creation, eventually the shit hit the fan.\n We naively thought that using the good old👵 ThreadLocal\u0026lt;'T\u0026gt; was the simplest and easiest way to go to keep this one IModel instance per thread.\nAnyhoo, we kept investigating🔬 to tackle this IModel issue in a concurrent context, and found a few articles (two of the links below were written by Mike Hadlow, the author of the EasyNetQ library):\n Very high latency for GC when using (loads of) ThreadLocal EasyNetQ: A Breaking Change, IPublishChannel How Do I Detect When a Client Thread Exits?  The point made by Mike👇:\n Initially I\u0026rsquo;d hoped to hide channels from EasyNetQ users🙈. With subscriptions this makes sense - after the client creates a subscription, EasyNetQ looks after the channel and the subscription consuming thread. But publishing turned out to be more complex. Channels cannot be shared between threads, so EasyNetQ initially used a ThreadLocal channel for all publish calls☎️. A new thread meant a new channel🌄.\nThis problem manifested itself with a nasty operational bug we suffered. We had a service which woke up on a timer, created some threads, and published messages on them. Although the threads ended, the channels didn\u0026rsquo;t close📭, and each timer interval🕰️ added another few open channels.\n So ok\u0026hellip; it seems that just relying on the vanilla edition of ThreadLocal\u0026lt;'T\u0026gt;🍨 might not do, mostly cause this concurrency primitive does not handle the exiting thread scenario.\nFair enough, we kept googling🔎 and found these two little gems💎 drafted by Ayende👨‍🏫:\n The slow slowdown of large systems The design and implementation of a better ThreadLocal\u0026lt;T\u0026gt;  As well this library📚:\n UnmanagedThreadUtils  We started to draft our own implementation of ThreadLocal\u0026lt;'T\u0026gt; supporting the thread exiting scenario when carrying a IDisposable resource, a bit like what is described in this SO answer.\n\u0026ldquo;It works\u0026hellip; but not on my machine!\u0026rdquo; (the usual Natalie 🙋‍♀️) At this stage, we were fairly confident that we had a working solution to solve our initial problem. But spoiler alert: we did not and we were once again prooved all wrong🙅‍♀️, once again. While we were reviewing our new solution before shipping it to our integration environment, we quickly realized that there was something off when using a sizable amount of data. In fact, I still kept having timeouts⏰ while my other colleagues did not. We were all testing using the same codebase and the same Docker configuration🐋\u0026hellip; so by all accounts we were supposed to get the same results and\u0026hellip; and we were all wondering \u0026ldquo;How is that even possible?\u0026rdquo;\n\u0026ldquo;Hey Cap\u0026rsquo;, looks like it\u0026rsquo;s related to your hardware\u0026rdquo; 💽👩‍✈️ Fair enough, yes, my laptop💻 is infamously slow when compared to the machines🖥️ of my teammates.\nIt didn\u0026rsquo;t take us too long to realize that given different pieces of hardware (RAM, CPU) accessed (i.e. limited) either thru a VM or a Docker Container🐳 mean they still can have vastly different performances. It\u0026rsquo;s then easy to understand that the underlying hardware powering this or that bit of infra is likely to improve or worsen the capability to support a bigger number of concurrent writers. If your hardware falls short, it\u0026rsquo;s very likely that you\u0026rsquo;re going to have a lock-bottleneck at some point.\nBetter Parallelism, an attempt ↗️🧵 One way to tackle the overall concurrency per layer of infra was to adjust using the TPL DataFlow - TPL. It goes without saying that different environment had different infra constraints.\n\u0026ldquo;Btw, if it\u0026rsquo;s really just a timeout problem, can\u0026rsquo;t you just increase the value of the so-called timeout?\u0026rdquo; ↗️⏳ Yes and we did on multiple occasions. Also one more thing, it wasn\u0026rsquo;t just about a Hangfire job, we also have an admin api so-to-speak to restart the job we mentioned early on and it was expected to run within 5 minutes (or as fast as possible). The thing is that it wasn\u0026rsquo;t just a timeout thing, or even just about parallelism and what we already listed above, cause\u0026hellip;\nBig File (You Are Beautiful!) 💾 \u0026hellip; basically here is the deal, we needed to juggled with all these intertwined bits of parameters:\n PostgreSQL: Timeout (we want that as fast as possible)⏰ RabbitMQ: Timeout (same as above)⏰ RabbitMQ: Single IModel per Thread policy👮‍♀️ Maximum degree of parallelism per \u0026ldquo;IO layer\u0026rdquo; 🔀 The respective bits of hardware / VM / Docker containers supporting a given service and relevant bits of infrastructure dependencies💪  The overall problem looked like a Gordian knot🧶 until we realized that we could bypass it altogether with a different approach\u0026hellip; when we started to a have closer look at\u0026hellip; well\u0026hellip; the files💾 and more precisely, their average size.\nIn retrospect, persisting \u0026ldquo;relatively big files\u0026rdquo; 100MB either to a JSONB column🧮 or to RabbitMQ (tho for the RabbitMQ part there were first split based on some business conditions) wasn\u0026rsquo;t really the best option available out there\u0026hellip; performance-wise🐢.\nSolution 🌟 Given all the information above, it became crystal clear🔮 that the main hiccup was to persist files where they were not really supposed to be persisted\u0026hellip; so we decided to use\u0026hellip;\n(\u0026ldquo;a\u0026rdquo; Corporate[i.e. on-premise]) S3 ☁️ Our first and original \u0026ldquo;sin\u0026quot;🛐 was purely architectural📐. Removing the file persistence💾 from PostgreSQL and RabbitMQ \u0026ldquo;duties\u0026rdquo; and hence keeping just a reference(i.e. ID) to the file in the S3 bucket💿 did alleviate a lot the IO burden by several orders of magnitude.\nWhat we have done is essentially delegating⏏️ the infra burden to another service that is good at handling file persistence and \u0026ldquo;voila\u0026quot;🍷🥖.\nNote: I didn\u0026rsquo;t mention it when I first drafted this article, but if you consider that several instances of the same service are running simulatenously and given that the even store is a very central building block in our architecture it makes sense to NOT burden it with long concurrent writes.\nEasyNetQ 🐰⌨️ This issue (and the investigations we did around this issue) made us realize another problem. When we started to use RabbitMQ in our project we thought we would need a certain granularity hence the decision to pick the rather low-level RabbitMQ .NET official client\u0026hellip;\nWe had this premise, because in our past experience with Entity Framework🧰, the overly complex abstraction made us litterally miserable😿 and we were spending (i.e. wasting?) more time fighting⚔️ the framework than benefiting from it (i.e. you deliver less business-value).\nBut keep in mind that the opposite is also true (i.e. afaik, these days, the vast majority of the software developers on Earth aren\u0026rsquo;t coding in assembly). There is a middle-ground, but it changes over time, due to skills, deadlines, priorities, and so forth.\nSo\u0026hellip; I hate to break it to you dear reader\u0026hellip; but contrary to the popular belief🌎 that \u0026ldquo;it\u0026rsquo;s always better to have a close-to-the-metal🔧 implementation that you can gradually customize and tune accordingly to your very needs\u0026rdquo; (and illustrated in articles like this one, or that one), it seems that well it\u0026rsquo;s not always that \u0026ldquo;straightforward\u0026rdquo; to figure out who or more exactly what is actually doing the heavy lifting.\nMaintaining our messaging implementation✉️ with the official RabbitMQ .NET client with all its caveats had a cost and it became increasingly expensive to keep up with all the new features and the ones that are going to be released.\nThe value💰 we bring to our project is not measured by the time we spent on tweaking the official RabbitMQ .NET client by doing the heavy lifting ourselves but rather by providing business-value to our stakeholders and end-users.\n\u0026ldquo;It\u0026rsquo;s all your f*cking fault!\u0026rdquo; or \u0026ldquo;Amateur-hour, Admit it! You just made a monumental mistake!\u0026rdquo; 🙇‍♀️ It could be very tempting to draw this kind of conclusion, but the truth is \u0026ldquo;nope\u0026rdquo;, nawt really, sorry pal, I do mean it.\nFun facts:\n We actually even had a branch with the S3 solution ready to be merged a few months ago📅, but we didn\u0026rsquo;t know business priorities would change that much over time and the priority was given to other more important topics🤑. We started with EasyNetQ back in the day, but the configuration system wasn\u0026rsquo;t too appealing and we were afraid that we would have been stuck with this lib at some point, i.e. a situation similar to a vendor lock-in🔒.  The Cheesy🧀-Corny🍿 Moment⌛ (also known as The Emotional Natalie Moment🤸‍♀️, but let\u0026rsquo;s just pretend it\u0026rsquo;s actually wholesome🧸).\nYou can always regret that you haven\u0026rsquo;t done enough. But here are my cheap 2 cents👛: it\u0026rsquo;s pointless. You can\u0026rsquo;t know everything in advance and making wild guesses about the future doesn\u0026rsquo;t seem like a reasonable option either. You learn what you can learn from something that didn\u0026rsquo;t pan out as good as you expected it to be, but it doesn\u0026rsquo;t mean that the decisions you made back then were all just plain wrong.\nYour decision process is just as contextual as the constraints involved when you were making those very same decisions at a given time📆. It is unproductive to place the blame on a context.\nI would even go as far as to say that blaming is probably the thing we should all probably refrain ourselves from doing. Instead we would all be much better off focusing on finding solutions and actionable tasks to solve our problems rather than just getting pissed at the circumstances and going round in circles.\n The devil😈 is in the details.\n   ","permalink":"https://natalie-o-perret.github.io/posts/2-2021-02-15-post-mortem-how-to-handle-big-files-with-rabbitmq-and-postgresql/","summary":"TL;DR ⏱️ Long story short: 90%-out-of-the-blue chance that you just don\u0026rsquo;t.\nIf you want more details about our story, then feel free to keep reading 👓⬇️.\nContext 🖼️ A not-so-long-while🦖 ago🦕 we had some alerts📟🔔 in our dedicated Slack Channel💬, triggered every now and then when a particular HangFire job was running:\n\u0026#34;XXXXXXX DataLoader\u0026#34; \u0026#34;Production\u0026#34; Error 2021-01-22 09:04 SpanId: \u0026#34;xxxxxxxxxxxxx\u0026#34; TraceId: \u0026#34;xxxxxxxxxxxxx\u0026#34; An unhandled exception occurred during the Hangfire job. Depth 0: System.","title":"Post-mortem✨💀: How To Handle Big Files With RabbitMQ🐇 and PostgreSQL🐘?"},{"content":"A relatively long time ago 📅, in a not-so-far-away galaxy 🌌 an acquaintance of mine challenged me to start writing something like a personal journal 📖, and quite frankly, for some reason I can\u0026rsquo;t even fathom now, I couldn\u0026rsquo;t really help but to accept that challenge ✅.\nAnd here it is, how I pretty much ended up creating this diary-slash-blog thingy 🤔.\nI am now trying to live up to my very own words 💬, like some sort of grown-up woman 👵. Though, I can\u0026rsquo;t really promise writing on a regular daily, weekly, monthly, bi-annual(❓) basis (is that really a thing? Well, I guess it is now). Regardless, I\u0026rsquo;ve found the practice of my broken 🩹 writing style 🖋️ interesting nonetheless.\nAlso, I like the idea of sharing some ahem-teresting video 🎞️ at the end of each post, so there\u0026rsquo;s that too.\n  ","permalink":"https://natalie-o-perret.github.io/posts/1-2020-08-14-challenge-accepted/","summary":"A relatively long time ago 📅, in a not-so-far-away galaxy 🌌 an acquaintance of mine challenged me to start writing something like a personal journal 📖, and quite frankly, for some reason I can\u0026rsquo;t even fathom now, I couldn\u0026rsquo;t really help but to accept that challenge ✅.\nAnd here it is, how I pretty much ended up creating this diary-slash-blog thingy 🤔.\nI am now trying to live up to my very own words 💬, like some sort of grown-up woman 👵.","title":"Challenge Accepted ✅ "},{"content":"Me(h)ow-dy y\u0026rsquo;all! 🐱🐮 I am Natalie, a Software Developer 👩‍💻 currently based in Paris and working @ Veepee. I talk and write about work, legal stuff, ethics, mental health, and software development.\nWhat I\u0026rsquo;m up to these days 🗓️🤹‍♀️  🔭 I\u0026rsquo;m currently working on F# stuff at work and in my spare time. 🌱 I\u0026rsquo;m currently learning Rust 👯 I\u0026rsquo;m looking to collaborate on F#, C#, Rust 🤸‍♀️ I\u0026rsquo;m looking for help with Rust 💬 You can ask me about .NET, C# or F# 📫 How to reach me: Twitter or LinkedIn, my DMs are open. 👩 Pronouns: ♀️she/her/hers♀️ ⚡ Fun fact: I lived in quite a few places on Earth, as a result my carbon footprint is pretty huge (and I\u0026rsquo;m not proud about it).  Things I enjoy 😽😻  (⚠️spoiler-alert) coding 💻 stuff that, may or may not, bring some value to others the annoyingly glowing purple colour 💜 the arts 🎶✒️🍿💃🗿🎨 hiking 🚶‍♀️🥾 supporting LGBTQ2+ indivdiuals 🏳️‍🌈⚢⚣⚤⚥⚧️⚦ debating using the principle of charity ⚖️ learning stuff 📚 the damp weather ⛈️ almost all sorts of food,🤤 preferrably Asian cuisine 🥢🍜🦐🍛🥔 taking sh*te 🖼️ with my 📱📷 travelling 🧳🗺️🧭 even if it means carbon footprint++; ✈️🚆🚴‍♀️ and\u0026hellip; not necessarily in that order.  Oh and I occasionally write some bad random blog posts here on this blog rumbling about all kinds of things, often plagued with typos🖊️, unapologetically in, plain, broken English,📚 sprinkled with an unreasonable amount of commas.\nWhat kind of music do I enjoy listening to? 🎵🎶 All kinds of music, really.\nSpotify 🟢  SoundCloud 🟠 Natalie Perret · Share Anything else❓ No, not really, ah yea, maybe just this random thingy below 🤷‍♀️:\n  ","permalink":"https://natalie-o-perret.github.io/about/","summary":"Me(h)ow-dy y\u0026rsquo;all! 🐱🐮 I am Natalie, a Software Developer 👩‍💻 currently based in Paris and working @ Veepee. I talk and write about work, legal stuff, ethics, mental health, and software development.\nWhat I\u0026rsquo;m up to these days 🗓️🤹‍♀️  🔭 I\u0026rsquo;m currently working on F# stuff at work and in my spare time. 🌱 I\u0026rsquo;m currently learning Rust 👯 I\u0026rsquo;m looking to collaborate on F#, C#, Rust 🤸‍♀️ I\u0026rsquo;m looking for help with Rust 💬 You can ask me about .","title":"About🆔"}]